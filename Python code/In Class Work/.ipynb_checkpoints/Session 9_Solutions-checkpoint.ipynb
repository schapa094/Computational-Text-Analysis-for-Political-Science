{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session Outline\n",
    "\n",
    "Go through the rest of session 8:\n",
    "\n",
    "- Word Sense Disambiguation \n",
    "- Named Entity Recognition\n",
    "- Entity Linking\n",
    "\n",
    "New Stuff:\n",
    "\n",
    "- Word-embeddings and Vector Space\n",
    "- Cosine Similarity\n",
    "\n",
    "Homework:\n",
    "- Session 8 excercises + Session 9 excercises = Homework today\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs, nltk, string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "exclude = set(string.punctuation)\n",
    "stop_word_list = stopwords.words('english')\n",
    "\n",
    "# input should be a string - we need a simple pipeline for getting word embeddings\n",
    "def nlp_simple_pipeline(text):\n",
    "    \n",
    "    #it depends if the words have been lowercased or not\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = nltk.word_tokenize(text)\n",
    "        \n",
    "    text = [token for token in text if token not in exclude and token.isalpha()]\n",
    "    \n",
    "    text = [token for token in text if token not in stop_word_list]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we know if words are related?\n",
    "\n",
    "Similarity: two words sharing a high number of salient (e.g., synonyms)\n",
    "Relatedness: two words semantically associated, without being necessarily similar (car and pilot)\n",
    "\n",
    "### Why do we need embeddings?\n",
    "\n",
    "- To capture the meaning of a word in the vector-space\n",
    "- words with similar context occupy close spatial positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Space Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic and naive method for transforming words into vectors is to count occurrence of each word in each document - **countvectorizing or one-hot encoding **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = [\n",
    "          'Text of first document.',\n",
    "          'Text of the second document made longer.',\n",
    "          'Number three.',\n",
    "          'This is number four.',\n",
    "]\n",
    "\n",
    "# learn the vocabulary and store CountVectorizer sparse matrix in X\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# columns of X correspond to the result of this method\n",
    "vectorizer.get_feature_names() == (\n",
    "    ['document', 'first', 'four', 'is', 'longer',\n",
    "     'made', 'number', 'of', 'second', 'text',\n",
    "     'the', 'this', 'three'])\n",
    "# retrieving the matrix in the numpy form\n",
    "X.toarray()\n",
    "\n",
    "\n",
    "# transforming a new document according to learn vocabulary\n",
    "#vectorizer.transform(['A new document.']).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The idea is to collect a set of documents (they can be words, sentences, paragraphs or even articles) and count the occurrence of every word in them. \n",
    "- Strictly speaking, the columns of the resulting matrix are words and the rows are documents.\n",
    "\n",
    "**This is a however sparse vector** - has mostly zero values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x13 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 17 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = vectorizer.fit_transform(corpus)\n",
    "X#make list of aall sentenced "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['document',\n",
       " 'first',\n",
       " 'four',\n",
       " 'is',\n",
       " 'longer',\n",
       " 'made',\n",
       " 'number',\n",
       " 'of',\n",
       " 'second',\n",
       " 'text',\n",
       " 'the',\n",
       " 'this',\n",
       " 'three']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x13 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 17 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordembeddings\n",
    "\n",
    "- Lower dimension dense vectors\n",
    "\n",
    "Word2vec - Train the neural network for two different tasks:\n",
    "- Predicting the word, given the context\n",
    "- Classification of a word, given another word in the sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can train your own embeddings: for example on political text, manifestos etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets see how we can use them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# the model is organized like this: word = embeddings\n",
    "small_model = gensim.models.KeyedVectors.load_word2vec_format('/Users/Ashrakat/Desktop/small-embeddings.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to see the embeddings of a word, you just do:\n",
    "\n",
    "print (small_model[\"clinton\"])\n",
    "print (small_model[\"obama\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_model.wv.most_similar(positive=['obama'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get relatedness\n",
    "\n",
    "print (small_model.wv.similarity('clinton', 'clinton'))\n",
    "print (small_model.wv.similarity('clinton', 'obama'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# you can represent the meaning of an article, by the average of their embeddings\n",
    "# let's compute the embeddings for an article\n",
    "\n",
    "dataset = codecs.open(\"/Users/Ashrakat/Desktop/rt_dataset.tsv\", \"r\", \"utf-8\").read().strip().split(\"\\n\")\n",
    "\n",
    "article = dataset[4].split(\"\\t\")[3]\n",
    "\n",
    "cleaned_article = nlp_simple_pipeline(article)\n",
    "print (cleaned_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each word, load embeddings\n",
    "for word in cleaned_article:\n",
    "    print (word)\n",
    "    embed_word = small_model[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling exceptions\n",
    "for word in cleaned_article:\n",
    "    try:\n",
    "        embed_word = small_model[word]\n",
    "    except KeyError:\n",
    "        print (word)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing the meaning of an article as a single embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def article_embedding(cleaned_article):\n",
    "    \n",
    "    article_embedd = []\n",
    "    # for each word in the article, you take the embeddings\n",
    "    for word in cleaned_article:\n",
    "        try:\n",
    "            embed_word = small_model[word]\n",
    "            article_embedd.append(embed_word)\n",
    "        except KeyError:\n",
    "            continue\n",
    "    \n",
    "    # average vectors of all words\n",
    "    avg = [float(sum(col))/len(col) for col in zip(*article_embedd)]\n",
    "    avg = np.array(avg).reshape(1, -1)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = dataset[1]\n",
    "cleaned_article = nlp_simple_pipeline(article)\n",
    "embed_art = article_embedding(cleaned_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine-similarity\n",
    "\n",
    "- Cosine similarity calculates similarity by measuring the cosine of angle between two vectors in a multi-dimensional space.\n",
    "- The smaller the angle the higher the cosine similarity\n",
    "- scikit learn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "image = mpimg.imread(\"/Users/Ashrakat/Desktop/cosines1.png\")\n",
    "plt.imshow(image)\n",
    "plt.gcf().set_size_inches(15, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Images used by Dhruvil Karani in his Medium article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "image = mpimg.imread(\"/Users/Ashrakat/Desktop/cosines.png\")\n",
    "plt.imshow(image)\n",
    "plt.gcf().set_size_inches(15, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nlp_simple_pipeline(text):\n",
    "    \n",
    "    #it depends if the words have been lowercased or not\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = nltk.word_tokenize(text)\n",
    "        \n",
    "    text = [token for token in text if token not in exclude and token.isalpha()]\n",
    "    \n",
    "    text = [token for token in text if token not in stop_word_list]\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def article_embedding(cleaned_article):\n",
    "    \n",
    "    article_embedd = []\n",
    "    # for each word in the article, you take the embeddings\n",
    "    for word in cleaned_article:\n",
    "        try:\n",
    "            embed_word = small_model[word]\n",
    "            article_embedd.append(embed_word)\n",
    "        except KeyError as e:\n",
    "            print (e,word)\n",
    "            continue\n",
    "    \n",
    "    # average vectors of all words\n",
    "    avg = [float(sum(col))/len(col) for col in zip(*article_embedd)]\n",
    "    avg = np.array(avg).reshape(1, -1)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Document_A=\"The sun is shining today I want to go out\"\n",
    "Document_B= \"What a beautiful summer day.\"\n",
    "Document_C= \"The winter is coming\"\n",
    "Document_D= \"It is snowing over here\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "doc_a=nlp_simple_pipeline(Document_A)\n",
    "doc_b=nlp_simple_pipeline(Document_B)\n",
    "doc_c=nlp_simple_pipeline(Document_C)\n",
    "doc_d=nlp_simple_pipeline(Document_D)\n",
    "\n",
    "doc_a=article_embedding(doc_a)\n",
    "doc_b=article_embedding(doc_b)\n",
    "doc_c=article_embedding(doc_c)\n",
    "doc_d=article_embedding(doc_d)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"compare a and b\",cosine_similarity(doc_a, doc_b))\n",
    "print(\"compare a and c\",cosine_similarity(doc_a, doc_c))\n",
    "print(\"compare c and d\",cosine_similarity(doc_c, doc_d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define that you need to exclude punctuation\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "# this represent any text as a single \"doc-embedding\" we use it both for the query and the sentences\n",
    "# input should be a string\n",
    "def text_embedding(text):\n",
    "    \n",
    "    #this works to lower text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # we tokenize the text in single words\n",
    "    text = nltk.tokenize.WordPunctTokenizer().tokenize(text)\n",
    "    \n",
    "    # we remove numbers and punctuation\n",
    "    text = [token for token in text if token not in exclude and token.isalpha()]\n",
    "    \n",
    "    doc_embed = []\n",
    "    \n",
    "    # for each word we get the embedding and we append it to a list\n",
    "    for word in text:\n",
    "            try:\n",
    "                embed_word = emb_model[word]\n",
    "                doc_embed.append(embed_word)\n",
    "            except KeyError as e: # if there is an error we continue\n",
    "                print (e,word)\n",
    "                continue\n",
    "    # we average the embeddings of all the words, getting an overall doc embedding\n",
    "    if len(doc_embed)>0:\n",
    "        avg = [float(sum(col))/len(col) for col in zip(*doc_embed)]\n",
    "\n",
    "        avg = np.array(avg).reshape(1, -1)\n",
    "\n",
    "        # the output is a doc-embedding\n",
    "        return avg\n",
    "    else:\n",
    "        return \"Empty\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_query = [\"crime\",\"criminal\",\"murder\",\"drugs\",\"rape\"]\n",
    "\n",
    "#query = [\" \".join(nlp_pipeline(\" \".join(text_query)))]\n",
    "\n",
    "emb_query = nlp_simple_pipeline(\" \".join(text_query))\n",
    "emb_query = article_embedding(\" \".join(text_query))\n",
    "\n",
    "emb_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "tsv_file = open(\"/Users/Ashrakat/Desktop/rt_dataset.tsv\")\n",
    "read_tsv = csv.reader(tsv_file, delimiter=\"\\t\")\n",
    "all_lines=[]\n",
    "for line in read_tsv:\n",
    "    print(line)\n",
    "    all_lines.append(line)\n",
    "tsv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lines=all_lines[1:101]\n",
    "len(all_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_corpus = [x+[text_embedding(x[3])] for x in all_lines] #what is x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs, nltk, string, os, gensim\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "scores = [x + [cosine_similarity(x[4], emb_query)[0]] for x in embs_corpus]\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspired by fede\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "image = mpimg.imread(\"/Users/Ashrakat/Desktop/screen.png\")\n",
    "plt.imshow(image)\n",
    "plt.gcf().set_size_inches(15, 10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise 1**\n",
    "\n",
    "Using Pandas and sascat_excerpts\n",
    "\n",
    "\n",
    "- Use the word embeddings on your content column --> create a new column that reflects the content with the word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise 2**\n",
    "\n",
    "Use the rt_dataset.tsv\n",
    "\n",
    "a)\n",
    "- use single function that takes a law/document (one row), cleans the article and creates the article-embeddings\n",
    "- you can only consider the first 100 articles for speed\n",
    "\n",
    "b)\n",
    "- create a dictionary of article embeddings [key number of article: value is the embeeding] (of course here you do that for the content)\n",
    "- bonus: only if you can: the dictionary should be nested. a dictionary within a dictionary. so that item 0/dictionary 0 will have a dictionary that has the embedding and the title of the artcile. for example they keys can be embed: value the embeedings etc. if you cant do this step its okay you can follow with the rest\n",
    "- extract the embedding of article 4\n",
    "- calculate the cosine similarity between that article (article 4) and all the values in your dictionary - save them in a list of list of lists [if you have created the title include it in this list]\n",
    "- sort your values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_doc_embedding(cleaned_article):\n",
    "    \n",
    "    # ....\n",
    "    \n",
    "    return doc_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_embedding(cleaned_article):\n",
    "    \n",
    "    article_embedd = []\n",
    "    # for each word in the article, you take the embeddings\n",
    "    for word in cleaned_article:\n",
    "        try:\n",
    "            embed_word = small_model[word]\n",
    "            article_embedd.append(embed_word)\n",
    "        except KeyError:\n",
    "            continue\n",
    "    \n",
    "    # average vectors of all words\n",
    "    avg = [float(sum(col))/len(col) for col in zip(*article_embedd)]\n",
    "    avg = np.array(avg).reshape(1, -1)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_embedding(cleaned_article):\n",
    "    \n",
    "    article_embedd = []\n",
    "    # for each word in the article, you take the embeddings\n",
    "    for word in cleaned_article:\n",
    "        try:\n",
    "            embed_word = small_model[word]\n",
    "            article_embedd.append(embed_word)\n",
    "        except KeyError:\n",
    "            continue\n",
    "    \n",
    "    # average vectors of all words\n",
    "    avg = [float(sum(col))/len(col) for col in zip(*article_embedd)]\n",
    "    avg = np.array(avg).reshape(1, -1)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dictionary of article embeddings\n",
    "articles_embeddings = {}\n",
    "\n",
    "# you can limit this to the first 100 articles with\n",
    "#for k in range(len(dataset[:100])):\n",
    "\n",
    "for k in range(len(dataset[:1000])):\n",
    "    article = dataset[k]\n",
    "    title = article.split(\"\\t\")[1]\n",
    "    cleaned_article = nlp_simple_pipeline(article)\n",
    "    embed_art = article_embedding(cleaned_article)\n",
    "    articles_embeddings[str(k)] = {\"title\":title,\"embed\":embed_art}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "our_article_embedd = articles_embeddings[\"4\"][\"embed\"]\n",
    "print (our_article_embedd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#This method takes either a vector array or a distance matrix, and returns a distance matrix. If the input is a vector array, the distances are computed. If the input is a distances matrix, it is returned instead.\n",
    "#to compute similarity\n",
    "\n",
    "ranking = []\n",
    "#values() returns a list of all the values available in a given dictionary.\n",
    "for article,values in articles_embeddings.items(): #Return an iterator over the dictionary's (key, value) \n",
    "    similarity_score = cosine_similarity(values[\"embed\"],our_article_embedd)[0][0] #first values\n",
    "    ranking.append([values[\"title\"],similarity_score])\n",
    "    \n",
    "print(ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking.sort(key=lambda x: x[1],reverse=True)\n",
    "\n",
    "ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for el,score in ranking[:10]:\n",
    "    print (el,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
