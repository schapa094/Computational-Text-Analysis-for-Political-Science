{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Ashrakat/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import *\n",
    "from numpy import random # random data\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import codecs\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pprint\n",
    "import random\n",
    "from urllib import request\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets take a look at what we have learned until now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = codecs.open(\"/Users/Ashrakat/Desktop/rt_dataset.tsv\", \"r\", \"utf-8\").read().strip().split(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "DAMN Fans took to Twitter in their droves to commend the team and their supporters for their classy performance that many credited as the highlight of Euro-2016 so far. 5\n",
      " \n",
      "Tokenizer:\n",
      " ['DAMN', 'Fans', 'took', 'to', 'Twitter', 'in', 'their', 'droves', 'to', 'commend', 'the', 'team', 'and', 'their', 'supporters', 'for', 'their', 'classy', 'performance', 'that', 'many', 'credited', 'as', 'the', 'highlight', 'of', 'Euro-2016', 'so', 'far', '.', '5']\n",
      " \n",
      "lower-cased:\n",
      " ['damn', 'fans', 'took', 'to', 'twitter', 'in', 'their', 'droves', 'to', 'commend', 'the', 'team', 'and', 'their', 'supporters', 'for', 'their', 'classy', 'performance', 'that', 'many', 'credited', 'as', 'the', 'highlight', 'of', 'euro-2016', 'so', 'far', '.', '5']\n",
      " \n",
      "without stopwords:\n",
      " ['damn', 'fans', 'took', 'twitter', 'droves', 'commend', 'team', 'supporters', 'classy', 'performance', 'many', 'credited', 'highlight', 'euro-2016', 'far', '.', '5']\n",
      " \n",
      "without punctuation:\n",
      " ['damn', 'fans', 'took', 'twitter', 'droves', 'commend', 'team', 'supporters', 'classy', 'performance', 'many', 'credited', 'highlight', 'euro-2016', 'far', '5']\n",
      " \n",
      "without punctuation and numbers:\n",
      " ['damn', 'fans', 'took', 'twitter', 'droves', 'commend', 'team', 'supporters', 'classy', 'performance', 'many', 'credited', 'highlight', 'far']\n"
     ]
    }
   ],
   "source": [
    "# take a single article\n",
    "article = dataset[50].split(\"\\t\")[3]\n",
    "\n",
    "\n",
    "# split into sentences\n",
    "sentences = nltk.sent_tokenize(article) \n",
    "\n",
    "# take one single sentence (I'm adding a number at the end - just for testing that stripping numbers work fine)\n",
    "sentence = sentences[4] + \" 5\"\n",
    "print(\" \")\n",
    "\n",
    "print (sentence)\n",
    "\n",
    "# word tokenizer\n",
    "sentence = nltk.word_tokenize(sentence)\n",
    "print(\" \")\n",
    "\n",
    "print (\"Tokenizer:\\n\",sentence)\n",
    "\n",
    "\n",
    "# lowering words\n",
    "sentence = [word.lower() for word in sentence]\n",
    "print(\" \")\n",
    "\n",
    "print (\"lower-cased:\\n\",sentence)\n",
    "\n",
    "\n",
    "# defining stop-words (it's a list)\n",
    "stop_word_list = stopwords.words('english')\n",
    "\n",
    "\n",
    "# removing stopwords\n",
    "sentence = [word for word in sentence if word not in stop_word_list]\n",
    "print(\" \")\n",
    "\n",
    "print (\"without stopwords:\\n\",sentence)\n",
    "\n",
    "\n",
    "# defining punctuation to be removed\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "#\n",
    "sentence = [token for token in sentence if token not in exclude]\n",
    "print(\" \")\n",
    "\n",
    "print (\"without punctuation:\\n\",sentence)\n",
    "\n",
    "\n",
    "#remove punctuations and numbers\n",
    "sentence = [word for word in sentence if word.isalpha()]\n",
    "print(\" \")\n",
    "print(\"without punctuation and numbers:\\n\",sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "## We reduce the word to its stem basically, we reduce words to their core root\n",
    "\n",
    "- Put in simple terms: several conditions that the stemmer evaluates\n",
    "- Mapping a group of words to the same stem\n",
    "- May result in non-words, rather stems of the actual words\n",
    "- chopping off the last part of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When can it be helpful\n",
    "\n",
    "- Information retrieval setting and you want to boost your algorithm’s recall. (Recall is defined as the number of relevant documents retrieved by a search divided by the total number of existing relevant documents)\n",
    "- Query Expansion: Search Environments which refers to that when a user inputs a query. It is used to expand or enhance the query to match additional documents\n",
    "- World usaga analysis in a corpus, we wish to condense words to reduce variabilitu.\n",
    "- Sentiment Analysis\n",
    "- Classification tasks\n",
    "\n",
    "## Take care of overstemming and understemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['univers', 'univers', 'univers', 'univers']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1=[\"university\", \"universal\", \"universities\", \"universe\"]\n",
    "stem_list1 = [snowball_stemmer.stem(word) for word in list1]\n",
    "stem_list1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'datum', 'date']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list2=[ \"data\" , \"datum\",\"date\"]\n",
    "stem_list2 = [snowball_stemmer.stem(word) for word in list2]\n",
    "stem_list2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference between the porter and lancaster stemming algorithms:\n",
    "- Three main stemming mechanisms iare **Porter, Snowball(Porter2), Lancaster**\n",
    "- Lancaster is more aggressive that porter\n",
    "- Snowball stemmer has applications in different European languages\n",
    "- Russian is surprisingly easy to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized:\n",
      " ['damn', 'fans', 'took', 'twitter', 'droves', 'commend', 'team', 'supporters', 'classy', 'performance', 'many', 'credited', 'highlight', 'far']\n",
      "stemmed:\n",
      " ['damn', 'fan', 'took', 'twitter', 'drove', 'commend', 'team', 'support', 'classi', 'perform', 'mani', 'credit', 'highlight', 'far']\n"
     ]
    }
   ],
   "source": [
    "# import the library\n",
    "from nltk.stem import SnowballStemmer\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "print (\"tokenized:\\n\",sentence)\n",
    "\n",
    "stem_sentence = [snowball_stemmer.stem(word) for word in sentence]\n",
    "\n",
    "print (\"stemmed:\\n\",stem_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "import re\n",
    "import pprint\n",
    "import random\n",
    "from urllib import request\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "article = \"\"\"Donald John Trump (born June 14, 1946) is the 45th and current president of the United States. Before entering politics, he was a businessman and television personality.\n",
    "Trump was born and raised in Queens, a borough of New York City, and received a bachelor's degree in economics from the Wharton School. He took charge of his family's real-estate business in 1971, renamed it The Trump Organization, and expanded its operations from Queens and Brooklyn into Manhattan. The company built or renovated skyscrapers, hotels, casinos, and golf courses. Trump later started various side ventures, mostly by licensing his name. He produced and hosted The Apprentice, a reality television series, from 2003 to 2015. As of 2020, Forbes estimated his net worth to be $2.1 billion.[a]\n",
    "Trump entered the 2016 presidential race as a Republican and defeated 16 other candidates in the primaries. His political positions have been described as populist, protectionist, and nationalist. Despite not being favored in most forecasts, he was elected over Democratic nominee Hillary Clinton, although he lost the popular vote. \"\"\"\n",
    "# from Wikipedia https://en.wikipedia.org/wiki/Donald_Trump\n",
    "\n",
    "tokens = word_tokenize(article)\n",
    "\n",
    "snowball = SnowballStemmer(\"english\")\n",
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "\n",
    "snowball_output = [snowball_stemmer.stem(t) for t in tokens]             \n",
    "porter_output = [porter.stem(t) for t in tokens]             \n",
    "lancaster_output = [lancaster.stem(t) for t in tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['donald', 'john', 'trump', '(', 'born', 'june', '14', ',', '1946', ')', 'is', 'the', '45th', 'and', 'current', 'presid', 'of', 'the', 'unit', 'state', '.', 'befor', 'enter', 'polit', ',', 'he', 'wa', 'a', 'businessman', 'and', 'televis', 'person', '.', 'trump', 'wa', 'born', 'and', 'rais', 'in', 'queen', ',', 'a', 'borough', 'of', 'new', 'york', 'citi', ',', 'and', 'receiv', 'a', 'bachelor', \"'s\", 'degre', 'in', 'econom', 'from', 'the', 'wharton', 'school', '.', 'He', 'took', 'charg', 'of', 'hi', 'famili', \"'s\", 'real-est', 'busi', 'in', '1971', ',', 'renam', 'it', 'the', 'trump', 'organ', ',', 'and', 'expand', 'it', 'oper', 'from', 'queen', 'and', 'brooklyn', 'into', 'manhattan', '.', 'the', 'compani', 'built', 'or', 'renov', 'skyscrap', ',', 'hotel', ',', 'casino', ',', 'and', 'golf', 'cours', '.', 'trump', 'later', 'start', 'variou', 'side', 'ventur', ',', 'mostli', 'by', 'licens', 'hi', 'name', '.', 'He', 'produc', 'and', 'host', 'the', 'apprentic', ',', 'a', 'realiti', 'televis', 'seri', ',', 'from', '2003', 'to', '2015', '.', 'As', 'of', '2020', ',', 'forb', 'estim', 'hi', 'net', 'worth', 'to', 'be', '$', '2.1', 'billion', '.', '[', 'a', ']', 'trump', 'enter', 'the', '2016', 'presidenti', 'race', 'as', 'a', 'republican', 'and', 'defeat', '16', 'other', 'candid', 'in', 'the', 'primari', '.', 'hi', 'polit', 'posit', 'have', 'been', 'describ', 'as', 'populist', ',', 'protectionist', ',', 'and', 'nationalist', '.', 'despit', 'not', 'be', 'favor', 'in', 'most', 'forecast', ',', 'he', 'wa', 'elect', 'over', 'democrat', 'nomine', 'hillari', 'clinton', ',', 'although', 'he', 'lost', 'the', 'popular', 'vote', '.']\n"
     ]
    }
   ],
   "source": [
    "print(porter_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['donald', 'john', 'trump', '(', 'born', 'june', '14', ',', '1946', ')', 'is', 'the', '45th', 'and', 'current', 'presid', 'of', 'the', 'unit', 'state', '.', 'befor', 'enter', 'polit', ',', 'he', 'was', 'a', 'businessman', 'and', 'televis', 'person', '.', 'trump', 'was', 'born', 'and', 'rais', 'in', 'queen', ',', 'a', 'borough', 'of', 'new', 'york', 'citi', ',', 'and', 'receiv', 'a', 'bachelor', \"'s\", 'degre', 'in', 'econom', 'from', 'the', 'wharton', 'school', '.', 'he', 'took', 'charg', 'of', 'his', 'famili', \"'s\", 'real-est', 'busi', 'in', '1971', ',', 'renam', 'it', 'the', 'trump', 'organ', ',', 'and', 'expand', 'it', 'oper', 'from', 'queen', 'and', 'brooklyn', 'into', 'manhattan', '.', 'the', 'compani', 'built', 'or', 'renov', 'skyscrap', ',', 'hotel', ',', 'casino', ',', 'and', 'golf', 'cours', '.', 'trump', 'later', 'start', 'various', 'side', 'ventur', ',', 'most', 'by', 'licens', 'his', 'name', '.', 'he', 'produc', 'and', 'host', 'the', 'apprentic', ',', 'a', 'realiti', 'televis', 'seri', ',', 'from', '2003', 'to', '2015', '.', 'as', 'of', '2020', ',', 'forb', 'estim', 'his', 'net', 'worth', 'to', 'be', '$', '2.1', 'billion', '.', '[', 'a', ']', 'trump', 'enter', 'the', '2016', 'presidenti', 'race', 'as', 'a', 'republican', 'and', 'defeat', '16', 'other', 'candid', 'in', 'the', 'primari', '.', 'his', 'polit', 'posit', 'have', 'been', 'describ', 'as', 'populist', ',', 'protectionist', ',', 'and', 'nationalist', '.', 'despit', 'not', 'be', 'favor', 'in', 'most', 'forecast', ',', 'he', 'was', 'elect', 'over', 'democrat', 'nomine', 'hillari', 'clinton', ',', 'although', 'he', 'lost', 'the', 'popular', 'vote', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(snowball_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['donald', 'john', 'trump', '(', 'born', 'jun', '14', ',', '1946', ')', 'is', 'the', '45th', 'and', 'cur', 'presid', 'of', 'the', 'unit', 'stat', '.', 'bef', 'ent', 'polit', ',', 'he', 'was', 'a', 'businessm', 'and', 'televid', 'person', '.', 'trump', 'was', 'born', 'and', 'rais', 'in', 'queen', ',', 'a', 'borough', 'of', 'new', 'york', 'city', ',', 'and', 'receiv', 'a', 'bachel', \"'s\", 'degr', 'in', 'econom', 'from', 'the', 'wharton', 'school', '.', 'he', 'took', 'charg', 'of', 'his', 'famy', \"'s\", 'real-estate', 'busy', 'in', '1971', ',', 'renam', 'it', 'the', 'trump', 'org', ',', 'and', 'expand', 'it', 'op', 'from', 'queen', 'and', 'brooklyn', 'into', 'manhat', '.', 'the', 'company', 'built', 'or', 'renov', 'skyscrap', ',', 'hotel', ',', 'casino', ',', 'and', 'golf', 'cours', '.', 'trump', 'lat', 'start', 'vary', 'sid', 'vent', ',', 'most', 'by', 'licens', 'his', 'nam', '.', 'he', 'produc', 'and', 'host', 'the', 'appr', ',', 'a', 'real', 'televid', 'sery', ',', 'from', '2003', 'to', '2015', '.', 'as', 'of', '2020', ',', 'forb', 'estim', 'his', 'net', 'wor', 'to', 'be', '$', '2.1', 'bil', '.', '[', 'a', ']', 'trump', 'ent', 'the', '2016', 'presid', 'rac', 'as', 'a', 'republ', 'and', 'def', '16', 'oth', 'candid', 'in', 'the', 'prim', '.', 'his', 'polit', 'posit', 'hav', 'been', 'describ', 'as', 'pop', ',', 'protect', ',', 'and', 'nat', '.', 'despit', 'not', 'being', 'fav', 'in', 'most', 'forecast', ',', 'he', 'was', 'elect', 'ov', 'democr', 'nomin', 'hil', 'clinton', ',', 'although', 'he', 'lost', 'the', 'popul', 'vot', '.']\n"
     ]
    }
   ],
   "source": [
    "print(lancaster_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Snowball Stemmer    lancaster Stemmer   \n",
      "friend              friend              friend              \n",
      "friendship          friendship          friend              \n",
      "friends             friend              friend              \n",
      "friendships         friendship          friend              \n",
      "stabil              stabil              stabl               \n",
      "destabilize         destabil            dest                \n",
      "misunderstanding    misunderstand       misunderstand       \n",
      "railroad            railroad            railroad            \n",
      "moonlight           moonlight           moonlight           \n",
      "football            footbal             footbal             \n"
     ]
    }
   ],
   "source": [
    "#A list of words to be stemmed\n",
    "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\"football\"]\n",
    "print(\"{0:20}{1:20}{2:20}\".format(\"Word\",\"Snowball Stemmer\",\"lancaster Stemmer\"))\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}{2:20}\".format(word,snowball_stemmer.stem(word),lancaster.stem(word)))\n",
    "    \n",
    "#https://www.datacamp.com/community/tutorials/stemming-lemmatization-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "## Setting words to their dictionary form\n",
    "The process of grouping together the inflected forms of a word as a single\n",
    "item.\n",
    "\n",
    "\n",
    "## Employing Wordnet - large lexical database of English\n",
    "\n",
    "read more about wordnet here: https://wordnet.princeton.edu/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When is it benefitial to lemmatize?\n",
    "\n",
    "- Topic modeling: relies on the distribution of words, the identification of which is dependent on a string match between words, which is achieved by lemmatizing their forms so that all variants are consistent across documents\n",
    "- Sentiment Analysis\n",
    "- Classification tasks\n",
    "\n",
    "- The general rule: if it does not help your performance, do not lemmatize.\n",
    "\n",
    "- Some sentiment analysis methods can have different performance according to lemmitazation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatization: ['fish', 'fishing', 'fish']\n",
      "stemming: ['fish', 'fish', 'fish']\n"
     ]
    }
   ],
   "source": [
    "list3=[\"fishes\", \"fishing\", \"fish\"]\n",
    "lemma_list3 = [wordnet_lemmatizer.lemmatize(word) for word in list3]\n",
    "lemma_list3\n",
    "\n",
    "stem_list3 = [snowball_stemmer.stem(word) for word in list3]\n",
    "stem_list3 # here we map a group of words to the same stem\n",
    "\n",
    "print(\"lemmatization:\", lemma_list3)\n",
    "print(\"stemming:\", stem_list3)# here we map a group of words to the same stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "party=[\"party\", \"partying\",\"parties\"]\n",
    "lemma_party = [wordnet_lemmatizer.lemmatize(word) for word in party]\n",
    "stem_party = [snowball_stemmer.stem(word) for word in party]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# see the difference it doesnt treat verbs for example similarly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatization: ['connection', 'connecting', 'connect', 'connected', 'connection']\n",
      "stemming: ['connect', 'connect', 'connect', 'connect', 'connect']\n"
     ]
    }
   ],
   "source": [
    "list4=[\"connections\",\"connecting\",\"connect\",\"connected\",\"connection\"]\n",
    "lemma_list4 = [wordnet_lemmatizer.lemmatize(word) for word in list4]\n",
    "stem_list4 = [snowball_stemmer.stem(word) for word in list4]\n",
    "print(\"lemmatization:\", lemma_list4)\n",
    "print(\"stemming:\", stem_list4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemmed:\n",
      " ['damn', 'fan', 'took', 'twitter', 'drove', 'commend', 'team', 'support', 'classi', 'perform', 'mani', 'credit', 'highlight', 'far']\n",
      " \n",
      "lemmatized:\n",
      " ['damn', 'fan', 'took', 'twitter', 'drove', 'commend', 'team', 'supporter', 'classy', 'performance', 'many', 'credited', 'highlight', 'far']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lemma_sent = [wordnet_lemmatizer.lemmatize(word) for word in sentence]\n",
    "stem_sentence = [snowball_stemmer.stem(word) for word in sentence]\n",
    "\n",
    "print (\"stemmed:\\n\", stem_sentence)\n",
    "print(\" \")\n",
    "print (\"lemmatized:\\n\",lemma_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Snowball Stemmer    Lemmatizing         \n",
      "friend              friend              friend              \n",
      "friendship          friendship          friendship          \n",
      "friends             friend              friend              \n",
      "friendships         friendship          friendship          \n",
      "stabil              stabil              stabil              \n",
      "destabilize         destabil            destabilize         \n",
      "misunderstanding    misunderstand       misunderstanding    \n",
      "railroad            railroad            railroad            \n",
      "moonlight           moonlight           moonlight           \n",
      "football            footbal             football            \n",
      "party               parti               party               \n",
      "extreme             extrem              extreme             \n",
      "muslim              muslim              muslim              \n",
      "islam               islam               islam               \n",
      "political           polit               political           \n",
      "politics            polit               politics            \n",
      "perfect             perfect             perfect             \n"
     ]
    }
   ],
   "source": [
    "#A list of words to be stemmed and lemmatized\n",
    "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\",\"destabilize\",\\\n",
    "             \"misunderstanding\",\"railroad\",\"moonlight\",\"football\",\"party\",\"extreme\",\\\n",
    "             \"muslim\",\"islam\",\"political\",\"politics\",\"perfect\"]\n",
    "print(\"{0:20}{1:20}{2:20}\".format(\"Word\",\"Snowball Stemmer\",\"Lemmatizing\"))\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}{2:20}\".format(word,snowball_stemmer.stem(word),wordnet_lemmatizer.lemmatize(word)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Article: Donald John Trump (born June 14, 1946) is the 45th and current president of the United States. Before entering politics, he was a businessman and television personality.\n",
      "Trump was born and raised in Queens, a borough of New York City, and received a bachelor's degree in economics from the Wharton School. He took charge of his family's real-estate business in 1971, renamed it The Trump Organization, and expanded its operations from Queens and Brooklyn into Manhattan. The company built or renovated skyscrapers, hotels, casinos, and golf courses. Trump later started various side ventures, mostly by licensing his name. He produced and hosted The Apprentice, a reality television series, from 2003 to 2015. As of 2020, Forbes estimated his net worth to be $2.1 billion.[a]\n",
      "Trump entered the 2016 presidential race as a Republican and defeated 16 other candidates in the primaries. His political positions have been described as populist, protectionist, and nationalist. Despite not being favored in most forecasts, he was elected over Democratic nominee Hillary Clinton, although he lost the popular vote. \n",
      "\n",
      "Original : was, New: wa\n",
      "Original : was, New: wa\n",
      "Original : its, New: it\n",
      "Original : operations, New: operation\n",
      "Original : skyscrapers, New: skyscraper\n",
      "Original : hotels, New: hotel\n",
      "Original : casinos, New: casino\n",
      "Original : courses, New: course\n",
      "Original : ventures, New: venture\n",
      "Original : as, New: a\n",
      "Original : candidates, New: candidate\n",
      "Original : primaries, New: primary\n",
      "Original : positions, New: position\n",
      "Original : as, New: a\n",
      "Original : forecasts, New: forecast\n",
      "Original : was, New: wa\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Original Article: %s' % (article))\n",
    "print()\n",
    "\n",
    "for token in tokens:\n",
    "    lemmatized_token = wordnet_lemmatizer.lemmatize(token)\n",
    "    \n",
    "    if token != lemmatized_token:\n",
    "        print('Original : %s, New: %s' % (token, lemmatized_token))\n",
    "        \n",
    "#inspired by edward ma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Always check if employing those methods will improve your performance!\n",
    "\n",
    "*Stemming can sometimes combine together words with substantively different meanings:<br\\>\n",
    "(“college students partying”, and “political parties”), which might be misleading in practice (Denny and Spirling 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Think about the order in your NLP Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise 1**\n",
    "\n",
    "Load the sascat_excerpt.tsv data. \n",
    "\n",
    "\n",
    "a) drop all columns but the id and the content \n",
    "\n",
    "b) Use pandas to create a column for each of the following:\n",
    "\n",
    "1. content - only lowercased\n",
    "2. content - lowercased, removed punctuation, removed numbers\n",
    "3. using the whole naive pipleline \n",
    "4. using a pipeline (usual text processing steps) with tokenizer\n",
    "5. using a pipeline (usual text processing steps) using tokenizer and stemmer (snowball)\n",
    "6. using a pipeline (usual text processing steps) using tokenizer and lemmetizer\n",
    "\n",
    "\n",
    "d) please write a paragraph using the Markdown function of what you think are the main differences between the 4 columns with text\n",
    "\n",
    "(it can be useful to write out this file to your harddrive to take a closer look)\n",
    "\n",
    "e) what are the most frequent 5 words using the last pipeline\n",
    "\n",
    "c) plot word frequency of these topics in a matplot\n",
    "\n",
    "e) write out this output as a comma seperated file for furture use.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise 2**\n",
    "\n",
    "Use this data you have just created.\n",
    "\n",
    "\n",
    "a) take a random sample of 1 row save them in a new dataframe called random\n",
    "\n",
    "b) keep only the rows that mention \"human right\" in the column where you only lower cased in our original dataset,\n",
    "\n",
    "c) take a random sample of 1 of those save the datafrae with the name \"hr\"\n",
    "\n",
    "e) create 6 lists from each of the columns [the 6 ways you have cleaned your data], print the content of all lists, do that for the dataframe random\n",
    "\n",
    "f) create 6 lists from each of the columns [the 6 ways you have cleaned your data], print the content of all lists, do that for the dataframe hr\n",
    "\n",
    "e) check overlap and difference between column 5 and column 6 (stemming and lemmatization pipelines) in the hr lists and in the random lists\n",
    "\n",
    "g) you see if you are writing a paper that needs to identify main topics for human rights, which of these processing mechanisms would you use and why?\n",
    "\n",
    "e) merge the rows of random and the row of hr - in a new dataframe called sanctions_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sources\n",
    "\n",
    "#Denny, M., & Spirling, A. (2018). Text Preprocessing For Unsupervised Learning: Why It Matters, When It Misleads, And What To Do About It. Political Analysis, 26(2), 168-189. doi:10.1017/pan.2017.44\n",
    "#https://towardsdatascience.com/stemming-lemmatization-what-ba782b7c0bd8\n",
    "#https://github.com/makcedward/nlp/blob/master/sample/nlp-stemming.ipynb\n",
    "#https://github.com/makcedward/nlp/blob/master/sample/nlp_lemmatization.ipynb\n",
    "#https://www.datacamp.com/community/tutorials/stemming-lemmatization-python\n",
    "#https://opendatagroup.github.io/data%20science/2019/03/21/preprocessing-text.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
