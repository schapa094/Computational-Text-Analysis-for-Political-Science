{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session Outline\n",
    "\n",
    "- Go through homework 5\n",
    "- Answers about Indexing\n",
    "- What is the Lambda operator?\n",
    "- A Note on **Visualization**\n",
    "- Cleaning Pipeline\n",
    "- Adding to the NLP Pipeline: **Tokenization**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![coding](https://media.giphy.com/media/FPbnShq1h1IS5FQyPD/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Needed Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Ashrakat/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import *\n",
    "from numpy import random # random data\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import codecs\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers about Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/Users/Ashrakat/Dropbox/University/Oxford/Jobs/Teaching/Text Analysis/code/Basics-of-Text-Analysis-for-Political-Science/Data/dataset_solution4_ex2.csv') as f:\n",
    "    lines = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if I want to keep year\n",
    "\n",
    "lines_year = lines.copy()\n",
    "lines_year[2]=lines_year[2][:-4]\n",
    "lines_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep only months\n",
    "lines_month = lines.copy()\n",
    "lines_month[2]=lines_month[2][4:6]\n",
    "lines_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep only days\n",
    "lines_day = lines.copy()\n",
    "lines_day[2]=lines_day[2][6:8]\n",
    "lines_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#or\n",
    "\n",
    "lines_day2 = lines.copy()\n",
    "lines_day2[2]=lines_day2[2][6:]\n",
    "lines_day2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the Lamda Operator?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A lambda function is a small anonymous function.\n",
    "\n",
    "`lambda arguments : expression`\n",
    "\n",
    "**Lets take a look at some examples?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 1 Lambda\n",
    "\n",
    "x = lambda a : a + 10\n",
    "print(x(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 2 Lambda\n",
    "\n",
    "x = lambda a, b : a * b\n",
    "print(x(5, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Indexing in Pandas\n",
    "#### Now I want a function to iterate through each row, and remove the last two charcters in the id-snippet column. Lambda can help do that\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sascat_ccounts = pd.read_csv('/Users/Ashrakat/Dropbox/University/Oxford/Jobs/Teaching/Text Analysis/code/Basics-of-Text-Analysis-for-Political-Science/Data/datafromsession5_exc3.tsv',delimiter=\"\\t\")\n",
    "del sascat_ccounts['Unnamed: 0']\n",
    "sascat_ccounts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sascat_ccounts['id-snippet'] = sascat_ccounts['id-snippet'].map(lambda x: str(x)[:-2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sascat_ccounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Note on Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "country_notflat = [a.split() for a in sascat_ccounts['mentions_countries']] #this puts them in a list of list\n",
    "country_notflat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sascat_ccounts = pd.read_csv('/Users/Ashrakat/Dropbox/University/Oxford/Jobs/Teaching/Text Analysis/code/Basics-of-Text-Analysis-for-Political-Science/Data/datafromsession5_exc3.tsv',delimiter=\"\\t\")\n",
    "del sascat_ccounts['Unnamed: 0']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- itertools is a function creating efficient loops, read more here https://docs.python.org/2/library/itertools.html and here https://realpython.com/python-itertools/\n",
    "\n",
    "- chains from itertools: chain('ABC', 'DEF') --> A B C D E F - This function takes any number of iterables as arguments and “chains” them together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_notflat = [a.split() for a in sascat_ccounts['mentions_countries']] \n",
    "#this command puts countries in a list of list according to thei structure in pandas\n",
    "country_notflat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(chain.from_iterable(country_notflat))\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort by values\n",
    "from collections import OrderedDict\n",
    "d_sorted_by_value = OrderedDict(sorted(counter.items(), key=lambda x: x[1]))\n",
    "d_sorted_by_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(d_sorted_by_value.keys(), d_sorted_by_value.values(), color='r')\n",
    "plt.gcf().set_size_inches(20, 10)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(x): #function to deal with a string\n",
    "    x=x.lower() # lower text\n",
    "    x=x.replace('[^\\w\\s]','') # remove punctuation\n",
    "    x=x.replace('\\d+', '') #replace digits with nothing\n",
    "    x=x.split() # split in words\n",
    "    x=[word for word in x if word not in stopwords.words('english')] #remove stopwords\n",
    "    #change that if youre using another languages\n",
    "    x=\" \".join(str(x) for x in x) # join in sentence\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization Session - using NLTK package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. Each of these smaller units are called tokens.\n",
    "\n",
    "- Tokanization has advantages over Python .split() function especially when it comes to sentence tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sentence tokenizer\n",
    "#inspired by Edward MA blogging on Medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article = 'In computer science, lexical analysis, lexing or tokenization is the process of \\\n",
    "converting a sequence of characters (such as in a computer program or web page) into a \\\n",
    "sequence of tokens (strings with an assigned and thus identified meaning). A program that \\\n",
    "performs lexical analysis may be termed a lexer, tokenizer,[1] or scanner, though scanner \\\n",
    "is also a term for the first stage of a lexer. A lexer is generally combined with a parser, \\\n",
    "which together analyze the syntax of programming languages, web pages, and so forth.'\n",
    "\n",
    "article2 = 'ConcateStringAnd123 ConcateSepcialCharacter_!@# !@#$%^&*()_+ 0123456'\n",
    "\n",
    "article3 = 'It is a great moment from 10 a.m. to 1 p.m. every weekend.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "for doc in [article, article2, article3]: # for every document \n",
    "    print('Original Article:', (doc)) #print the original article\n",
    "    print()#print empty space\n",
    "\n",
    "    sentences = re.split('(\\.|!|\\?)', doc) #split by these items using regex\n",
    "    \n",
    "    for i, s in enumerate(sentences): #count number of sentences\n",
    "        print('-->Sentence %d: %s' % (i, s)) #place holders %d\n",
    "\n",
    "        #different ways "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "print('NTLK Version: %s' % nltk.__version__)\n",
    "\n",
    "for article in [article, article2, article3]:\n",
    "    print('Original Article: %s' % (article))\n",
    "    print()\n",
    "\n",
    "    doc = sent_tokenize(article)\n",
    "    for i, token in enumerate(doc):\n",
    "        print('-->Sentence %d: %s' % (i, token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Amazing day... however, I still need a good night sleep. I will see you tomorrow for sure. Bye.Bye\"\"\"\n",
    "# Splits at '.' \n",
    "text.split('. ') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = nltk.sent_tokenize(text)\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets work with a new dataset and discover it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "dataset = codecs.open(\"/Users/Ashrakat/Desktop/rt_dataset.tsv\", \"r\", \"utf-8\").read().strip().split(\"\\n\") \n",
    "# we split by line breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[0])\n",
    "print(dataset[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# how to count the number of topics?\n",
    "\n",
    "topics = []\n",
    "for line in dataset:\n",
    "    topic = line.split(\"\\t\")[2]\n",
    "    topics.append(topic)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print (Counter(topics).most_common(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using list comprehension\n",
    "topics = [line.split(\"\\t\")[2] for line in dataset]\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "print (Counter(topics).most_common(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start with some real NLP\n",
    "\n",
    "# let's focus on a specific article, for example\n",
    "\n",
    "article = dataset[50].split(\"\\t\")[3]\n",
    "print (article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (type(article))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(article) # <-- documentation for this command: http://www.nltk.org/_modules/nltk/tokenize.html\n",
    "\n",
    "# for checking what you're getting back from a library, run these commands\n",
    "print (type(sentences))\n",
    "print (len(sentences))\n",
    "print (sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us consider a single sentence - how do we do that? ## use the 5th sentence\n",
    "\n",
    "sentence = sentences[4]\n",
    "print (sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's divide the sentence in tokens (aka single words)\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "print (tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower-casing the sentence\n",
    "without_capital_letters = [word.lower() for word in tokenized_sentence]\n",
    "\n",
    "print (without_capital_letters)\n",
    "\n",
    "# homework: write a for-loop for doing the same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "without_stop_words = [word for word in without_capital_letters if word not in stop]\n",
    "\n",
    "print (without_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "# homework: how do we exclude punctuation?, hint: use exclude, from the previous line\n",
    "\n",
    "without_punct = [word for word in without_stop_words if word not in exclude]\n",
    "\n",
    "print (without_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's take a look at our cleaning pipeline\n",
    "\n",
    "missing remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = codecs.open(\"/Users/Ashrakat/Desktop/rt_dataset.tsv\", \"r\", \"utf-8\").read().strip().split(\"\\n\") \n",
    "article=dataset[1]\n",
    "article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/Ashrakat/Desktop/rt_dataset.tsv') as f:\n",
    "    counter = Counter(f.read().strip().split())\n",
    "\n",
    "print(counter.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "exclude = set(string.punctuation)\n",
    "exclude.add(\"‘\")\n",
    "exclude.add(\"“\")\n",
    "\n",
    "import re\n",
    "\n",
    "def clean1(x): \n",
    "    x=x.replace('\\n\\n','') # remove the line breaks\n",
    "    x=x.lower()# lower text\n",
    "    x = ''.join(ch for ch in x if ch not in exclude) #remove punctuation\n",
    "    x=re.sub('[0-9]+', '', x) # remove numbers\n",
    "    x=x.split() #split words \n",
    "    x=[word for word in x if word not in stopwords.words('english')]#remove stopwords\n",
    "   #x=\" \".join(str(x) for x in x) # you can do this if you want to remove list structure\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned1=clean1(article)\n",
    "print(cleaned1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can you spot:\n",
    "- `httpstcohwuvvftbelarus`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nlp_pipeline1(text):\n",
    "    \n",
    "    # if you want you can split in sentences - i'm usually skipping this step\n",
    "    text=text.lower()\n",
    "    \n",
    "    #tokenize words for each sentence\n",
    "    text = nltk.word_tokenize(text)\n",
    "    \n",
    "    # remove punctuation and numbers\n",
    "    #text = [token for token in text if token.isalpha()]\n",
    "    \n",
    "    # remove stopwords - be careful with this step    \n",
    "    #text = [token for token in text if token not in stop_words]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned2=nlp_pipeline1(article)\n",
    "print(cleaned2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nlp_pipeline2(text):\n",
    "    \n",
    "    # if you want you can split in sentences - i'm usually skipping this step\n",
    "    text=text.lower()\n",
    "    \n",
    "    #tokenize words for each sentence\n",
    "    text = nltk.word_tokenize(text)\n",
    "    \n",
    "    # remove punctuation and numbers\n",
    "    text = [token for token in text if token.isalpha()]#The isalpha() keeps here caracters in the string are alphabets\n",
    "    \n",
    "    # remove stopwords - be careful with this step    \n",
    "    text = [token for token in text if token not in stop_words]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'article' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-298f899ebd52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcleaned2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnlp_pipeline2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'article' is not defined"
     ]
    }
   ],
   "source": [
    "cleaned2=nlp_pipeline2(article)\n",
    "print(cleaned2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#discrepencies between both lists\n",
    "def diff(first, second):\n",
    "        second = set(second)\n",
    "        return [item for item in first if item not in second]\n",
    "\n",
    "diff(cleaned1,cleaned2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- take care of weird punctuation in the beginning\n",
    "- we are going to see in sentiment analysis why dont etc. can be of important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise 1**\n",
    "\n",
    "- Discover what the filter function of lambdas is\n",
    "- use the list [listofnumbers] below\n",
    "- write a lamda expression that will only print in the list any number that is larger than or equal 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listofnumbers = [10,2,8,7,5,4,3,11,0, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter00 = filter (lambda x: x >= 5, listofnumbers) \n",
    "print(list(filter00))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise 2**\n",
    "\n",
    "- use data - datafromsession5_exc3 - you can find it on GitHub in the data folder\n",
    "- seperate in 3 new columns the following: a) the first six charachters for example hrxxxx  b) the number between the dash and dot for example 101 (in the first row) 3) the number after the dot (for example \"1\" in row 1)\n",
    "- call the new columns: id1 id2 id3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sascat_ccounts = pd.read_csv('/Users/Ashrakat/Dropbox/University/Oxford/Jobs/Teaching/Text Analysis/code/Basics-of-Text-Analysis-for-Political-Science/Data/datafromsession5_exc3.tsv',delimiter=\"\\t\")\n",
    "sascat_ccounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sascat_ccounts.rename(columns={'id-snippet':'id'}, \n",
    "                 inplace=True)\n",
    "sascat_ccounts['id3']= (sascat_ccounts[\"id\"].str[11:])\n",
    "sascat_ccounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sascat_ccounts['id1'], sascat_ccounts['id2_needsedit'] = sascat_ccounts['id'].str.split('-', 1).str\n",
    "sascat_ccounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sascat_ccounts['id2_wrong']=(sascat_ccounts[\"id2_needsedit\"].str[:-2]) #whats the mistake here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sascat_ccounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove everything after . is a better iption for creating id2\n",
    "sascat_ccounts['id2_correct'] = sascat_ccounts['id2_needsedit'].str.split('.').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sascat_ccounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sascat_ccounts['id2_correct'] = sascat_ccounts['id2_correct'].str.replace('.', '')\n",
    "sascat_ccounts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise 3**\n",
    "\n",
    "- Load convertfromwidetolong using pandas\n",
    "- Reshape dataset to long format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "reshapedata = pd.read_csv('/Users/Ashrakat/Desktop/convertfromwidetolong.tsv',delimiter=\"\\t\")\n",
    "reshapedata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshapedata1=reshapedata.melt(id_vars='country', var_name='year', value_name='some_value')\n",
    "reshapedata1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise 4**\n",
    "\n",
    "a)\n",
    "- Your first task is to improve the country dictionary from last class\n",
    "- Countries like the UK, can appear in the text as UK, United Kingdom, Great Britian etc. we dont account for all the ways the countries are mentioned\n",
    "- This is a huge problem in text analysis if you can not recognize different ways a country can be mentioned\n",
    "- create a new dictionary for Russia, that includes all the forms Russia can be mentioned\n",
    "- create the same type of dictionary for Central Republic of Congo\n",
    "- create the same type of dictionary for Yugoslavia\n",
    "\n",
    "then:\n",
    "\n",
    "- create three new columns, one for each Russia, one for CR of Congo, one for Yugoslavia.\n",
    "- in each of these columns add the name of the country that was mentioned.\n",
    "\n",
    "b)\n",
    "- Please use the methods for text processing that we have used until now to clean the articles, you can use pandas or any other method (think when does this step has to be done)\n",
    "\n",
    "c) \n",
    "- Count most frequent words overall\n",
    "- Plot most frequent words in a histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "countries = ['United States of America', 'Canada', 'Bahamas', 'Cuba', 'Haiti', 'Dominican Republic', 'Jamaica', 'Trinidad and Tobago', 'Barbados', 'Mexico', 'Belize', 'Guatemala', 'Honduras', 'El Salvador', 'Nicaragua', 'Costa Rica', 'Panama', 'Colombia', 'Venezuela', 'Guyana', 'Surinam', 'Ecuador', 'Peru', 'Brazil', 'Bolivia', 'Paraguay', 'Chile', 'Argentina', 'Uruguay', 'United Kingdom', 'Ireland', 'Netherlands', 'Belgium', 'Luxembourg', 'France', 'Switzerland', 'Spain', 'Portugal', 'German Federal Republic', 'German Democratic Republic', 'Poland', 'Austria', 'Hungary', 'Czechoslovakia', 'Czech Republic', 'Slovakia', 'Italy/Sardinia', 'Malta', 'Albania', 'Montenegro', 'Macedonia (Former Yugoslav Republic of)', 'Croatia', 'Serbia', 'Yugoslavia', 'Bosnia-Herzegovina', 'Kosovo', 'Slovenia', 'Greece', 'Cyprus', 'Bulgaria', 'Moldova', 'Rumania', 'Russia (Soviet Union)', 'Estonia', 'Latvia', 'Lithuania', 'Ukraine', 'Belarus (Byelorussia)', 'Armenia', 'Georgia', 'Azerbaijan', 'Finland', 'Sweden', 'Norway', 'Denmark', 'Iceland', 'Cape Verde', 'Guinea-Bissau', 'Equatorial Guinea', 'Gambia', 'Mali', 'Senegal', 'Benin', 'Mauritania', 'Niger', 'Cote D\\x92Ivoire', 'Guinea', 'Burkina Faso (Upper Volta)', 'Liberia', 'Sierra Leone', 'Ghana', 'Togo', 'Cameroon', 'Nigeria', 'Gabon', 'Central African Republic', 'Chad', 'Congo', 'Congo, Democratic Republic of (Zaire)', 'Uganda', 'Kenya', 'Tanzania/Tanganyika', 'Zanzibar', 'Burundi', 'Rwanda', 'Somalia', 'Djibouti', 'Ethiopia', 'Eritrea', 'Angola', 'Mozambique', 'Zambia', 'Zimbabwe (Rhodesia)', 'Malawi', 'South Africa', 'Namibia', 'Lesotho', 'Botswana', 'Swaziland', 'Madagascar', 'Comoros', 'Mauritius', 'Morocco', 'Algeria', 'Tunisia', 'Libya', 'Sudan', 'South Sudan', 'Iran (Persia)', 'Turkey (Ottoman Empire)', 'Iraq', 'Egypt', 'Syria', 'Lebanon', 'Jordan', 'Israel', 'Saudi Arabia', 'Yemen (Arab Republic of Yemen)', \"Yemen, People's Republic of\", 'Kuwait', 'Bahrain', 'Qatar', 'United Arab Emirates', 'Oman', 'Afghanistan', 'Turkmenistan', 'Tajikistan', 'Kyrgyz Republic', 'Uzbekistan', 'Kazakhstan', 'China', 'Tibet', 'Mongolia', 'Taiwan', \"Korea, People's Republic of\", 'Korea, Republic of', 'Japan', 'India', 'Bhutan', 'Pakistan', 'Bangladesh', 'Myanmar (Burma)', 'Sri Lanka (Ceylon)', 'Maldives', 'Nepal', 'Thailand', 'Cambodia (Kampuchea)', 'Laos', 'Vietnam, Democratic Republic of', 'Vietnam, Republic of', 'Malaysia', 'Singapore', 'Brunei', 'Philippines', 'Indonesia', 'East Timor', 'Australia', 'Papua New Guinea', 'New Zealand', 'Solomon Islands', 'Fiji']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yugoslavia = [\"Serbia\", \"Yugoslavia\", \"Serbia and Montenegro\"]\n",
    "CRC=[\"Congo\", \"Democratic\" \"Republic of Congo\", \"Zaire\"]\n",
    "russia=[\"Russia\", \"Russian Federation\", \"Soviets\", \"Soviet Union\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yugoslavia=[x.lower() for x in yugoslavia]\n",
    "CRC=[x.lower() for x in CRC]\n",
    "russia=[x.lower() for x in russia]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sascat = pd.read_csv('/Users/Ashrakat/Desktop/sascat_excerpt.tsv',delimiter=\"\\t\")\n",
    "sascat = sascat.rename({\"Unnamed: 10\": 'content'}, axis=1)\n",
    "sascat = sascat[['content','id-snippet']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nlp_pipeline(text):\n",
    "    \n",
    "    text=text.lower()\n",
    "    \n",
    "    #tokenize words for each sentence\n",
    "    text = nltk.word_tokenize(text)\n",
    "    \n",
    "    # remove punctuation and numbers\n",
    "    text = [token for token in text if token.isalpha()]#The isalpha() keeps here caracters in the string are alphabets\n",
    "    \n",
    "    # remove stopwords - be careful with this step    \n",
    "    text = [token for token in text if token not in stop_words]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>id-snippet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>provides enforcement provisions law impose san...</td>\n",
       "      <td>hr5114-101.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>requires delivery excess defense articles nato...</td>\n",
       "      <td>hr5114-101.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>prohibits making available esf foreign militar...</td>\n",
       "      <td>hr5114-101.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>prohibits obligation funds european bank recon...</td>\n",
       "      <td>hr5114-101.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>prohibits assistance countries fail take steps...</td>\n",
       "      <td>hr5114-101.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content    id-snippet\n",
       "0  provides enforcement provisions law impose san...  hr5114-101.1\n",
       "1  requires delivery excess defense articles nato...  hr5114-101.2\n",
       "2  prohibits making available esf foreign militar...  hr5114-101.3\n",
       "3  prohibits obligation funds european bank recon...  hr5114-101.4\n",
       "4  prohibits assistance countries fail take steps...  hr5114-101.5"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sascat['content'] = sascat['content'].apply(nlp_pipeline)\n",
    "sascat['content'] = [' '.join(map(str, l)) for l in sascat['content']] # remove list structure\n",
    "\n",
    "sascat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index, row in sascat.iterrows(): # itereate on rows\n",
    "    words_to_keep = [] # create a list of words you want to keep\n",
    "    for word in row[0].split(' '): # for words in each row in colum zero, split by space\n",
    "        if word in russia : #for each word in the dictionary\n",
    "            words_to_keep.append(word + ' ') # append to the list of words to keep the matching words\n",
    "    sascat.loc[index, 'mentions_russia']= ''.join(words_to_keep) #create a new column and paste the words you want to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in sascat.iterrows(): # itereate on rows\n",
    "    words_to_keep = [] # create a list of words you want to keep\n",
    "    for word in row[0].split(' '): # for words in each row in colum zero, split by space\n",
    "        if word in CRC : #for each word in the dictionary\n",
    "            words_to_keep.append(word + ' ') # append to the list of words to keep the matching words\n",
    "    sascat.loc[index, 'mentions_crc']= ''.join(words_to_keep) #create a new column and paste the words you want to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index, row in sascat.iterrows(): # itereate on rows\n",
    "    words_to_keep = [] # create a list of words you want to keep\n",
    "    for word in row[0].split(' '): # for words in each row in colum zero, split by space\n",
    "        if word in yugoslavia : #for each word in the dictionary\n",
    "            words_to_keep.append(word + ' ') # append to the list of words to keep the matching words\n",
    "    sascat.loc[index, 'mentions_yugoslavia']= ''.join(words_to_keep) #create a new column and paste the words you want to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sascat['mentions_russia']=sascat['mentions_russia'].str.split(' ').apply(set).str.join(', ')\n",
    "sascat['mentions_crc']=sascat['mentions_crc'].str.split(' ').apply(set).str.join(', ')\n",
    "sascat['mentions_yugoslavia']=sascat['mentions_yugoslavia'].str.split(' ').apply(set).str.join(', ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sascat['mentions_russia'] = sascat['mentions_russia'].apply(lambda x: x.replace(',', ''))\n",
    "sascat['mentions_crc'] = sascat['mentions_crc'].apply(lambda x: x.replace(',', ''))\n",
    "sascat['mentions_yugoslavia'] = sascat['mentions_yugoslavia'].apply(lambda x: x.replace(',', ''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>id-snippet</th>\n",
       "      <th>mentions_russia</th>\n",
       "      <th>mentions_crc</th>\n",
       "      <th>mentions_yugoslavia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>provides enforcement provisions law impose san...</td>\n",
       "      <td>hr5114-101.1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>requires delivery excess defense articles nato...</td>\n",
       "      <td>hr5114-101.2</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>yugoslavia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>prohibits making available esf foreign militar...</td>\n",
       "      <td>hr5114-101.3</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>prohibits obligation funds european bank recon...</td>\n",
       "      <td>hr5114-101.4</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>prohibits assistance countries fail take steps...</td>\n",
       "      <td>hr5114-101.5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>earmarks esf development assistance available ...</td>\n",
       "      <td>hr5368-102.16</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>expresses sense congress recommended levels es...</td>\n",
       "      <td>hr5368-102.17</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>prohibits esf assistance zaire</td>\n",
       "      <td>hr5368-102.18</td>\n",
       "      <td></td>\n",
       "      <td>zaire</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>limits amount esf assistance tied aid credits ...</td>\n",
       "      <td>hr5368-102.19</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>withholds specified amount funds allocated dom...</td>\n",
       "      <td>hr5368-102.20</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              content     id-snippet  \\\n",
       "0   provides enforcement provisions law impose san...   hr5114-101.1   \n",
       "1   requires delivery excess defense articles nato...   hr5114-101.2   \n",
       "2   prohibits making available esf foreign militar...   hr5114-101.3   \n",
       "3   prohibits obligation funds european bank recon...   hr5114-101.4   \n",
       "4   prohibits assistance countries fail take steps...   hr5114-101.5   \n",
       "..                                                ...            ...   \n",
       "95  earmarks esf development assistance available ...  hr5368-102.16   \n",
       "96  expresses sense congress recommended levels es...  hr5368-102.17   \n",
       "97                     prohibits esf assistance zaire  hr5368-102.18   \n",
       "98  limits amount esf assistance tied aid credits ...  hr5368-102.19   \n",
       "99  withholds specified amount funds allocated dom...  hr5368-102.20   \n",
       "\n",
       "   mentions_russia mentions_crc mentions_yugoslavia  \n",
       "0                                                    \n",
       "1                                        yugoslavia  \n",
       "2                                                    \n",
       "3                                                    \n",
       "4                                                    \n",
       "..             ...          ...                 ...  \n",
       "95                                                   \n",
       "96                                                   \n",
       "97                        zaire                      \n",
       "98                                                   \n",
       "99                                                   \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sascat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise 4**\n",
    "\n",
    "Use the cleaning methods we have learned until now to clean the text of rt_dataset.tsv\n",
    "\n",
    "You can use pandas or not, you're choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>topic</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16 Sep, 2016 14:08</td>\n",
       "      <td>Putin: We don’t approve of WADA hackers, but i...</td>\n",
       "      <td>news</td>\n",
       "      <td>We don’t approve of what hackers do, but what ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11 Sep, 2016 22:33</td>\n",
       "      <td>Hillary Clinton diagnosed with pneumonia, canc...</td>\n",
       "      <td>usa</td>\n",
       "      <td>Dr. Lisa Bardack, Clinton’s personal doctor s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2 Dec, 2016 20:15</td>\n",
       "      <td>Ronaldinho and Riquelme offer to come out of r...</td>\n",
       "      <td>sport</td>\n",
       "      <td>READ MORE: 71 dead after plane carrying Brazil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9 Feb, 2016 21:13</td>\n",
       "      <td>NATO &amp; European leaders whip up hysteria over ...</td>\n",
       "      <td>news</td>\n",
       "      <td>“The leaders of NATO member states and a numbe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5 Apr, 2016 18:01</td>\n",
       "      <td>US ‘Gremlin’ drones designed to cause missile ...</td>\n",
       "      <td>usa</td>\n",
       "      <td>Four firms, including fighter jet manufacturer...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date                                              title  \\\n",
       "0  16 Sep, 2016 14:08   Putin: We don’t approve of WADA hackers, but i...   \n",
       "1  11 Sep, 2016 22:33   Hillary Clinton diagnosed with pneumonia, canc...   \n",
       "2   2 Dec, 2016 20:15   Ronaldinho and Riquelme offer to come out of r...   \n",
       "3   9 Feb, 2016 21:13   NATO & European leaders whip up hysteria over ...   \n",
       "4   5 Apr, 2016 18:01   US ‘Gremlin’ drones designed to cause missile ...   \n",
       "\n",
       "   topic                                            content  \n",
       "0   news  We don’t approve of what hackers do, but what ...  \n",
       "1    usa   Dr. Lisa Bardack, Clinton’s personal doctor s...  \n",
       "2  sport  READ MORE: 71 dead after plane carrying Brazil...  \n",
       "3   news  “The leaders of NATO member states and a numbe...  \n",
       "4    usa  Four firms, including fighter jet manufacturer...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "rt_news = pd.read_csv('/Users/Ashrakat/Desktop/rt_dataset.tsv',delimiter=\"\\t\")\n",
    "rt_news.columns = ['date', 'title',\"topic\",\"content\"]\n",
    "rt_news.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_news['title_content']= rt_news['title']+ \" \" + rt_news['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "def nlp_pipeline2(text):\n",
    "    \n",
    "    text=text.lower()\n",
    "\n",
    "    #tokenize words for each sentence\n",
    "    text = nltk.word_tokenize(text)\n",
    "    \n",
    "    # remove punctuation and numbers\n",
    "    text = [token for token in text if token.isalpha()]#The isalpha() keeps here caracters in the string are alphabets\n",
    "    \n",
    "    # remove stopwords - be careful with this step    \n",
    "    text = [token for token in text if token not in stop_words]\n",
    "    \n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_news[\"title_content\"]=rt_news['title_content'][0:5000].apply(nlp_pipeline2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>topic</th>\n",
       "      <th>content</th>\n",
       "      <th>title_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16 Sep, 2016 14:08</td>\n",
       "      <td>Putin: We don’t approve of WADA hackers, but i...</td>\n",
       "      <td>news</td>\n",
       "      <td>We don’t approve of what hackers do, but what ...</td>\n",
       "      <td>[putin, approve, wada, hackers, information, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11 Sep, 2016 22:33</td>\n",
       "      <td>Hillary Clinton diagnosed with pneumonia, canc...</td>\n",
       "      <td>usa</td>\n",
       "      <td>Dr. Lisa Bardack, Clinton’s personal doctor s...</td>\n",
       "      <td>[hillary, clinton, diagnosed, pneumonia, cance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2 Dec, 2016 20:15</td>\n",
       "      <td>Ronaldinho and Riquelme offer to come out of r...</td>\n",
       "      <td>sport</td>\n",
       "      <td>READ MORE: 71 dead after plane carrying Brazil...</td>\n",
       "      <td>[ronaldinho, riquelme, offer, come, retirement...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9 Feb, 2016 21:13</td>\n",
       "      <td>NATO &amp; European leaders whip up hysteria over ...</td>\n",
       "      <td>news</td>\n",
       "      <td>“The leaders of NATO member states and a numbe...</td>\n",
       "      <td>[nato, european, leaders, whip, hysteria, myth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5 Apr, 2016 18:01</td>\n",
       "      <td>US ‘Gremlin’ drones designed to cause missile ...</td>\n",
       "      <td>usa</td>\n",
       "      <td>Four firms, including fighter jet manufacturer...</td>\n",
       "      <td>[us, gremlin, drones, designed, cause, missile...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date                                              title  \\\n",
       "0  16 Sep, 2016 14:08   Putin: We don’t approve of WADA hackers, but i...   \n",
       "1  11 Sep, 2016 22:33   Hillary Clinton diagnosed with pneumonia, canc...   \n",
       "2   2 Dec, 2016 20:15   Ronaldinho and Riquelme offer to come out of r...   \n",
       "3   9 Feb, 2016 21:13   NATO & European leaders whip up hysteria over ...   \n",
       "4   5 Apr, 2016 18:01   US ‘Gremlin’ drones designed to cause missile ...   \n",
       "\n",
       "   topic                                            content  \\\n",
       "0   news  We don’t approve of what hackers do, but what ...   \n",
       "1    usa   Dr. Lisa Bardack, Clinton’s personal doctor s...   \n",
       "2  sport  READ MORE: 71 dead after plane carrying Brazil...   \n",
       "3   news  “The leaders of NATO member states and a numbe...   \n",
       "4    usa  Four firms, including fighter jet manufacturer...   \n",
       "\n",
       "                                       title_content  \n",
       "0  [putin, approve, wada, hackers, information, l...  \n",
       "1  [hillary, clinton, diagnosed, pneumonia, cance...  \n",
       "2  [ronaldinho, riquelme, offer, come, retirement...  \n",
       "3  [nato, european, leaders, whip, hysteria, myth...  \n",
       "4  [us, gremlin, drones, designed, cause, missile...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sources: https://medium.com/@makcedward/nlp-pipeline-sentence-tokenization-part-6-86ed55b185e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
